# -*- coding: utf-8 -*-
"""Copy of gpt2_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STq_8BFLvd2hQh7PSB8Qb17shg3zZo2T
"""

import os
os.environ["WANDB_DISABLED"] = "true"

from google.colab import drive
drive.mount('/content/drive')

!pip install -U transformers datasets

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Fix padding issue
tokenizer.pad_token = tokenizer.eos_token

from datasets import load_dataset
from transformers import DataCollatorForLanguageModeling

# Load dataset using the datasets library
dataset = load_dataset("text", data_files={"train": "mydata.txt"})

# Data collator - handles batching and padding
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # GPT-2 uses causal (not masked) language modeling
)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], return_attention_mask=False)

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=4,  # Adjust num_proc based on your system's cores
    remove_columns=["text"],
)

# Further process the tokenized dataset to create blocks of text
block_size = 16

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.
    # We could add padding if the model supports it instead of dropping this remainder, but we build samples of fixed size here.
    # You can adjust block size here
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_dataset = tokenized_dataset.map(
    group_texts,
    batched=True,
    num_proc=4, # Adjust num_proc based on your system's cores
)

train_dataset = lm_dataset["train"]

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    per_device_train_batch_size=2,
    num_train_epochs=3,
    save_steps=500,
    save_total_limit=2,
    logging_steps=100,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

# ðŸ”¥ Train the model
trainer.train()

import os
from datasets import load_dataset
from transformers import GPT2Tokenizer

# Check if file exists
print("âœ… File exists:", os.path.exists("mydata.txt"))

# Show lines in file
with open("mydata.txt", "r") as f:
    lines = f.readlines()
print("ðŸ“„ Number of lines in mydata.txt:", len(lines))

# Load dataset
dataset = load_dataset("text", data_files={"train": "mydata.txt"})
print("ðŸ“¦ Raw dataset entries:", len(dataset["train"]))

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Tokenize
def tokenize_function(examples):
    return tokenizer(examples["text"], return_attention_mask=False)

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)
print("ðŸ”¤ Tokenized dataset samples:", len(tokenized_dataset["train"]))

# Group into blocks
block_size = 64
def group_texts(examples):
    concatenated = {k: sum(examples[k], []) for k in examples}
    total_len = (len(concatenated["input_ids"]) // block_size) * block_size
    result = {
        k: [t[i:i+block_size] for i in range(0, total_len, block_size)]
        for k, t in concatenated.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_dataset = tokenized_dataset.map(group_texts, batched=True)
print("ðŸ“Š Final training blocks (usable):", len(lm_dataset["train"]))

# Load model and tokenizer from saved directory
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2-finetuned-salik")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-finetuned-salik")

# Function to test multiple prompts
def generate_responses(prompts, model, tokenizer, max_tokens=50):
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.8,
            top_k=50,
            top_p=0.95
        )
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ðŸŸ¢ Prompt: {prompt}")
        print(f"ðŸ”· Response: {generated_text}\n{'-'*50}")

# âœ… Test with multiple prompts
prompts = [
    "What is your name?",
    "Tell me a joke.",
    "iam going to goa",
    "How's the weather today?",
    "Can you help me with math homework?"
]

generate_responses(prompts, model, tokenizer)

print(type(model))

model = trainer.model

model.save_pretrained("gpt2-finetuned-salik")
tokenizer.save_pretrained("gpt2-finetuned-salik")

import os
print(os.listdir("gpt2-finetuned-salik"))

import shutil
shutil.make_archive("gpt2-finetuned-salik", 'zip', "gpt2-finetuned-salik")
from google.colab import files
files.download("gpt2-finetuned-salik.zip")

!pip install gradio

from transformers import GPT2LMHeadModel, GPT2Tokenizer

model_path = "gpt2-finetuned-salik"
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.8,
        top_k=50,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

import gradio as gr

iface = gr.Interface(
    fn=generate_response,
    inputs=gr.Textbox(lines=2, placeholder="Ask me something..."),
    outputs="text",
    title="Fine-tuned GPT-2 Chatbot",
    description="Talk to a GPT-2 model fine-tuned on custom conversational data."
)

iface.launch()